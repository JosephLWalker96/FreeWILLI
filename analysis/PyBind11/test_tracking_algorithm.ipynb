{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b803f229-1f61-4be2-bc07-337912c2ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from scipy.spatial.distance import cdist\n",
    "import time\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys\n",
    "sys.path.append('build')  # Add the 'build' directory to the Python path\n",
    "\n",
    "import dbscan_module  # Import the shared object file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4d89df-d675-4ae4-ae40-e9eaee06ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kalman_update_point(cluster_points, cluster_center):\n",
    "    # Assuming cluster_points is a 2D numpy array with x and y coordinates\n",
    "    #print(cluster_points[-5:])\n",
    "    num_points = cluster_points.shape[0]\n",
    "    # Generate a range of colors that go from light to dark\n",
    "    colors = np.linspace(0.3, 1, num_points)  # Adjust 0.3 to 1 for darker shades\n",
    "    \n",
    "    # Plot the cluster points with varying darkness\n",
    "    scatter = plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors, cmap='Greys')\n",
    "    \n",
    "    # Plot the cluster center\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], color='red', marker='x', label='Cluster Center')\n",
    "    \n",
    "    # Add a colorbar, which will automatically use the 'Greys' colormap\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Point Intensity (later points darker)')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_dbscan_output(df, labels):\n",
    "    #print(\"###############################\")\n",
    "    #print(unique_labels)\n",
    "    global global_counter\n",
    "    global_counter +=1\n",
    "    print(\"DBSCAN counter: \",global_counter)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axs[0,0].scatter(df[\"PeakTime\"], df[\"El.\"], marker='o', c=labels)\n",
    "    axs[0,1].scatter(df[\"PeakTime\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,0].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "\n",
    "    inds = labels > -1\n",
    "    axs[1,1].scatter(df[\"El.\"][inds], df[\"Az.\"][inds], marker='o', c=labels[inds])\n",
    "    # Extract x and y axis limits\n",
    "    x_limits = axs[1, 1].get_xlim()\n",
    "    y_limits = axs[1, 1].get_ylim()\n",
    "    axs[1,1].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,1].set_xlim(x_limits)\n",
    "    axs[1,1].set_ylim(y_limits)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def PrintInfo(distance_matrix, associations, unassigned_clusters):\n",
    "    print(\"#########################################\")\n",
    "    print(\"distance matrix\")\n",
    "    print(distance_matrix)\n",
    "    print(f\"associations: {associations}\")\n",
    "    print(f\"unassigned_clusters: {unassigned_clusters}\")\n",
    "    for c in unassigned_clusters:\n",
    "        print(\"Initializing new filter for cluster: \", c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b36eaaa-da1b-4861-ae47-87027550fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-05T09:01:29.619200000 2023-11-05 09:01:59.619200\n",
      "length:  21\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:01:59.619200 2023-11-05 09:02:29.619200\n",
      "2023-11-05 09:02:29.619200 2023-11-05 09:02:59.619200\n",
      "length:  42\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:02:59.619200 2023-11-05 09:03:29.619200\n",
      "2023-11-05 09:03:29.619200 2023-11-05 09:03:59.619200\n",
      "length:  71\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:03:59.619200 2023-11-05 09:04:29.619200\n",
      "2023-11-05 09:04:29.619200 2023-11-05 09:04:59.619200\n",
      "length:  57\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:04:59.619200 2023-11-05 09:05:29.619200\n",
      "2023-11-05 09:05:29.619200 2023-11-05 09:05:59.619200\n",
      "length:  45\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:05:59.619200 2023-11-05 09:06:29.619200\n",
      "2023-11-05 09:06:29.619200 2023-11-05 09:06:59.619200\n",
      "length:  46\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:06:59.619200 2023-11-05 09:07:29.619200\n",
      "2023-11-05 09:07:29.619200 2023-11-05 09:07:59.619200\n",
      "length:  100\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:07:59.619200 2023-11-05 09:08:29.619200\n",
      "2023-11-05 09:08:29.619200 2023-11-05 09:08:59.619200\n",
      "length:  135\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:08:59.619200 2023-11-05 09:09:29.619200\n",
      "2023-11-05 09:09:29.619200 2023-11-05 09:09:59.619200\n",
      "length:  149\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:09:59.619200 2023-11-05 09:10:29.619200\n",
      "2023-11-05 09:10:29.619200 2023-11-05 09:10:59.619200\n",
      "length:  143\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:10:59.619200 2023-11-05 09:11:29.619200\n",
      "2023-11-05 09:11:29.619200 2023-11-05 09:11:59.619200\n",
      "length:  150\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:11:59.619200 2023-11-05 09:12:29.619200\n",
      "2023-11-05 09:12:29.619200 2023-11-05 09:12:59.619200\n",
      "length:  148\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:12:59.619200 2023-11-05 09:13:29.619200\n",
      "2023-11-05 09:13:29.619200 2023-11-05 09:13:59.619200\n",
      "length:  158\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:13:59.619200 2023-11-05 09:14:29.619200\n",
      "2023-11-05 09:14:29.619200 2023-11-05 09:14:59.619200\n",
      "length:  134\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:14:59.619200 2023-11-05 09:15:29.619200\n",
      "2023-11-05 09:15:29.619200 2023-11-05 09:15:59.619200\n",
      "length:  100\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:15:59.619200 2023-11-05 09:16:29.619200\n",
      "2023-11-05 09:16:29.619200 2023-11-05 09:16:59.619200\n",
      "length:  85\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:16:59.619200 2023-11-05 09:17:29.619200\n",
      "2023-11-05 09:17:29.619200 2023-11-05 09:17:59.619200\n",
      "length:  89\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:17:59.619200 2023-11-05 09:18:29.619200\n",
      "2023-11-05 09:18:29.619200 2023-11-05 09:18:59.619200\n",
      "length:  97\n",
      "-1 days +23:59:59.999970\n",
      "2023-11-05 09:18:59.619200 2023-11-05 09:19:29.619200\n",
      "2023-11-05 09:19:29.619200 2023-11-05 09:19:59.619200\n",
      "length:  79\n",
      "-1 days +23:59:59.999970\n",
      "sleeping....\n",
      "done....\n",
      "0.0010738372802734375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a program that performs multi-target tracking. \n",
    "Initially, clustering (DBSCAN) is performed on a batch of samples to estimate the number of targets. \n",
    "A kalman filter is then initialized for each cluster (i.e. target).\n",
    "New observations are used to either update an existing kalman filter's state or ignored as noise.\n",
    "Periodically, batches of observations are used to perform clustering to update the number of targets. \n",
    "Each cluster is either associated with an existing kalman filter or used to initialize a new kalman filter.\n",
    "If a kalman filter is not assocaited with a cluster for a specified number of clustering iterations, the filter is destroyed.\n",
    "\"\"\"\n",
    "global_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "# Load the data as per your existing code\n",
    "\"\"\"\n",
    "column_names = [\n",
    "    \"PeakTime\", \"Energy\", \"El.\", \"Az.\", \"TDOA12\", \"TDOA13\", \"TDOA14\",\n",
    "    \"TDOA23\", \"TDOA24\", \"TDOA34\", \"Xcorr12\", \"Xcorr13\", \"Xcorr14\",\n",
    "    \"Xcorr23\", \"Xcorr24\", \"Xcorr34\"\n",
    "]\n",
    "\"\"\"\n",
    "column_names = [\n",
    "    \"PeakTime\", \"Amplitude\", \"DOA_x\", \"DOA_y\", \"DOA_z\", \"TDOA12\", \"TDOA13\", \"TDOA14\",\n",
    "    \"TDOA23\", \"TDOA24\", \"TDOA34\", \"Xcorr12\", \"Xcorr13\", \"Xcorr14\",\n",
    "    \"Xcorr23\", \"Xcorr24\", \"Xcorr34\"\n",
    "]\n",
    "\n",
    "# Replace with your actual data path\n",
    "path_doa = \"../../C/deployment_files/2024-8-29-18-37-36-480140_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-9-25-13-5-15-419096_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-10-1-15-19-31-643613_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-10-1-15-55-32-804596_detection\" #track154\n",
    "#path_doa = \"../2024-9-12-9-36-14-51349_detection\"\n",
    "path_doa = \"../../C/deployment_files/new3.txt\"\n",
    "path_doa = \"../../../listener_program/tracks/231105_090101_000000_tracker_132\"\n",
    "\n",
    "#df = pd.read_csv(path_doa, delim_whitespace=True, names=column_names, skiprows=1)\n",
    "df = pd.read_csv(path_doa, sep=',', names=column_names, skiprows=1)\n",
    "\n",
    "# Instantiate the tracker\n",
    "#tracker = Tracker(eps=3, min_samples=15, missed_update_threshold=4)\n",
    "#tracker = dbscan_module.Tracker(3, 15, 4)\n",
    "#print('Before tracker')\n",
    "tracker = dbscan_module.Tracker(0.04, 15, 4)\n",
    "#print('After tracker')\n",
    "# Main processing loop (outside the class)\n",
    "#time_intervals = np.linspace(df[\"usec_since_Unix_Start\"].min(), df[\"usec_since_Unix_Start\"].max(), num=20)\n",
    "# Convert 'PeakTime' column to datetime if needed; assuming the timestamp is in microseconds since epoch\n",
    "timestamps = pd.to_datetime(df['PeakTime'], unit='us')\n",
    "\n",
    "# Determine the start and end time based on the data\n",
    "start_time = np.array(timestamps).min()\n",
    "end_time = np.array(timestamps).max()\n",
    "\n",
    "# Create 60-second intervals from start_time to end_time\n",
    "# Create 60-second intervals starting from start_time until an interval end time exceeds end_time\n",
    "time_intervals = [start_time]\n",
    "while time_intervals[-1] + pd.Timedelta(seconds=30) <= end_time:\n",
    "    time_intervals.append(time_intervals[-1] + pd.Timedelta(seconds=30))\n",
    "\n",
    "grouped_time_intervals = []\n",
    "\n",
    "# Iterate through each interval and extract observations\n",
    "cluster_runtimes = 0\n",
    "for ind in range(len(time_intervals) - 1):\n",
    "    # Get start and end times for the current interval\n",
    "    interval_start = time_intervals[ind]\n",
    "    interval_end = time_intervals[ind + 1]\n",
    "\n",
    "    print(interval_start,interval_end)\n",
    "    \n",
    "    # Filter observations within the current interval\n",
    "    observations = df[\n",
    "        (timestamps > interval_start) &\n",
    "        (timestamps <= interval_end)\n",
    "    ]\n",
    "\n",
    "    # If there are no data points in this interval, skip to the next iteration\n",
    "    if observations.empty:\n",
    "        continue\n",
    "    #print(\"hello\")\n",
    "    # Run DBSCAN on even intervals\n",
    "    if ind % 2 == 0:\n",
    "        #print(\"here\", ind)\n",
    "         \n",
    "        if ind != 0:\n",
    "            for _, row in observations.iterrows():\n",
    "                #observation = [row[\"El.\"], row[\"Az.\"]]\n",
    "                observation = [row[\"DOA_x\"], row[\"DOA_y\"], row[\"DOA_z\"]]\n",
    "                observation = np.squeeze(observation)\n",
    "                tracker.update_kalman_filters_continuous(observation, int(row['PeakTime']))\n",
    "                last_row = row\n",
    "        #print(\"ind\", ind)\n",
    "        #print(time_intervals[ind])\n",
    "\n",
    "        grouped_time_intervals.append([observations['PeakTime'].min(), observations['PeakTime'].max()])\n",
    "        \n",
    "        X = observations[[\"DOA_x\", \"DOA_y\", \"DOA_z\"]].values.astype(np.float32)\n",
    "        #print(\"Length:\")\n",
    "        print(\"length: \", len(X))\n",
    "        time_diff_seconds = (time_intervals[ind] - time_intervals[ind +1]) / 1e6  # 1e6 to convert microseconds to seconds\n",
    "        print(time_diff_seconds)\n",
    "        bef_clust = time.time()\n",
    "        tracker.process_batch(X) # Handle batch updates\n",
    "        af_clust = time.time()\n",
    "        runtime= af_clust - bef_clust\n",
    "        cluster_runtimes += runtime\n",
    "        #k = k+1\n",
    "    else:\n",
    "        # For odd intervals, handle individual sample updates\n",
    "        for _, row in observations.iterrows():\n",
    "            observation = [row[\"DOA_x\"], row[\"DOA_y\"], row[\"DOA_z\"]]\n",
    "            observation = np.squeeze(observation)\n",
    "            tracker.update_kalman_filters_continuous(observation, int(row['PeakTime']))\n",
    "            last_row = row\n",
    "\n",
    "print(\"sleeping....\")\n",
    "time.sleep(10)\n",
    "observation = [last_row[\"DOA_x\"], last_row[\"DOA_y\"], last_row[\"DOA_z\"]]\n",
    "observation = np.squeeze(observation)\n",
    "tracker.update_kalman_filters_continuous(observation, int(last_row['PeakTime']))\n",
    "print(\"done....\")\n",
    "print(cluster_runtimes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybind11_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
