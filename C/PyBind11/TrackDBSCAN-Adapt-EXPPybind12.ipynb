{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b803f229-1f61-4be2-bc07-337912c2ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from scipy.spatial.distance import cdist\n",
    "import time\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys\n",
    "sys.path.append('build')  # Add the 'build' directory to the Python path\n",
    "\n",
    "import dbscan_module  # Import the shared object file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4d89df-d675-4ae4-ae40-e9eaee06ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kalman_update_point(cluster_points, cluster_center):\n",
    "    # Assuming cluster_points is a 2D numpy array with x and y coordinates\n",
    "    #print(cluster_points[-5:])\n",
    "    num_points = cluster_points.shape[0]\n",
    "    # Generate a range of colors that go from light to dark\n",
    "    colors = np.linspace(0.3, 1, num_points)  # Adjust 0.3 to 1 for darker shades\n",
    "    \n",
    "    # Plot the cluster points with varying darkness\n",
    "    scatter = plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors, cmap='Greys')\n",
    "    \n",
    "    # Plot the cluster center\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], color='red', marker='x', label='Cluster Center')\n",
    "    \n",
    "    # Add a colorbar, which will automatically use the 'Greys' colormap\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Point Intensity (later points darker)')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_dbscan_output(df, labels):\n",
    "    #print(\"###############################\")\n",
    "    #print(unique_labels)\n",
    "    global global_counter\n",
    "    global_counter +=1\n",
    "    print(\"DBSCAN counter: \",global_counter)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axs[0,0].scatter(df[\"PeakTime\"], df[\"El.\"], marker='o', c=labels)\n",
    "    axs[0,1].scatter(df[\"PeakTime\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,0].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "\n",
    "    inds = labels > -1\n",
    "    axs[1,1].scatter(df[\"El.\"][inds], df[\"Az.\"][inds], marker='o', c=labels[inds])\n",
    "    # Extract x and y axis limits\n",
    "    x_limits = axs[1, 1].get_xlim()\n",
    "    y_limits = axs[1, 1].get_ylim()\n",
    "    axs[1,1].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,1].set_xlim(x_limits)\n",
    "    axs[1,1].set_ylim(y_limits)\n",
    "    plt.show()\n",
    "\n",
    "# Extract variables\n",
    "def plot_log(kalman_log):\n",
    "    times = []\n",
    "    filter_ids = []\n",
    "    predicted_xs = []\n",
    "    predicted_ys = []\n",
    "    \n",
    "    for entry in kalman_log:\n",
    "        times.append(entry.time)\n",
    "        filter_ids.append(entry.filter_id)\n",
    "        predicted_xs.append(entry.updated_x)\n",
    "        predicted_ys.append(entry.updated_y)\n",
    "    \n",
    "    filter_ids = np.array(filter_ids)\n",
    "    unique_ids = np.unique(filter_ids)\n",
    "    print(\"unique tracks: \", unique_ids)\n",
    "    \n",
    "    # Create a custom colormap for only the used unique filter IDs\n",
    "    num_unique = len(unique_ids)  # Number of unique filter IDs\n",
    "    colormap = plt.get_cmap('tab10', num_unique)  # Generate a colormap for the number of unique IDs\n",
    "    \n",
    "    # Normalize the colormap for the unique filter IDs\n",
    "    norm = mcolors.Normalize(vmin=min(unique_ids), vmax=max(unique_ids))\n",
    "    \n",
    "    # Display scatter plot for predicted_ys\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    scatter1 = ax1.scatter(times, predicted_ys, c=filter_ids, cmap=colormap, norm=norm, alpha=0.1)\n",
    "    ax1.set_ylim(-180, 200)\n",
    "    \n",
    "    # Create a ScalarMappable for the color bar to use only the available filter IDs\n",
    "    sm1 = ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm1.set_array([])  # Required for ScalarMappable\n",
    "    \n",
    "    # Add a colorbar with only the colors used in the plot\n",
    "    cbar1 = fig.colorbar(sm1, ax=ax1, ticks=unique_ids, label=\"Track IDs\")\n",
    "    \n",
    "    # Shade the regions specified by grouped_time_intervals\n",
    "    for interval in grouped_time_intervals:\n",
    "        ax1.axvspan(interval[0], interval[1], color='gray', alpha=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Display scatter plot for predicted_xs\n",
    "    fig, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    scatter2 = ax2.scatter(times, predicted_xs, c=filter_ids, cmap=colormap, norm=norm, alpha=0.1)\n",
    "    ax2.set_ylim(30, 200)\n",
    "    \n",
    "    # Create a ScalarMappable for the second color bar to use only the available filter IDs\n",
    "    sm2 = ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm2.set_array([])\n",
    "    \n",
    "    # Add a colorbar with only the colors used in the plot\n",
    "    cbar2 = fig.colorbar(sm2, ax=ax2, ticks=unique_ids, label=\"Track IDs\")\n",
    "    \n",
    "    # Shade the regions specified by grouped_time_intervals\n",
    "    for interval in grouped_time_intervals:\n",
    "        ax2.axvspan(interval[0], interval[1], color='gray', alpha=0.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def PrintInfo(distance_matrix, associations, unassigned_clusters):\n",
    "    print(\"#########################################\")\n",
    "    print(\"distance matrix\")\n",
    "    print(distance_matrix)\n",
    "    print(f\"associations: {associations}\")\n",
    "    print(f\"unassigned_clusters: {unassigned_clusters}\")\n",
    "    for c in unassigned_clusters:\n",
    "        print(\"Initializing new filter for cluster: \", c)\n",
    "\n",
    "\n",
    "def read_log_csv(filename):\n",
    "    \"\"\"\n",
    "    Reads the kalman_log CSV file and returns a list of LogEntry namedtuples.\n",
    "    \"\"\"\n",
    "    LogEntry = namedtuple('LogEntry', ['time', 'filter_id', 'updated_x', 'updated_y'])\n",
    "    kalman_log = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            entry = LogEntry(\n",
    "                time=float(row['time']),\n",
    "                filter_id=int(row['filter_id']),\n",
    "                updated_x=float(row['updated_x']),\n",
    "                updated_y=float(row['updated_y'])\n",
    "            )\n",
    "            kalman_log.append(entry)\n",
    "    return kalman_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36eaaa-da1b-4861-ae47-87027550fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a program that performs multi-target tracking. \n",
    "Initially, clustering (DBSCAN) is performed on a batch of samples to estimate the number of targets. \n",
    "A kalman filter is then initialized for each cluster (i.e. target).\n",
    "New observations are used to either update an existing kalman filter's state or ignored as noise.\n",
    "Periodically, batches of observations are used to perform clustering to update the number of targets. \n",
    "Each cluster is either associated with an existing kalman filter or used to initialize a new kalman filter.\n",
    "If a kalman filter is not assocaited with a cluster for a specified number of clustering iterations, the filter is destroyed.\n",
    "\"\"\"\n",
    "global_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "# Load the data as per your existing code\n",
    "\"\"\"\n",
    "column_names = [\n",
    "    \"PeakTime\", \"Energy\", \"El.\", \"Az.\", \"TDOA12\", \"TDOA13\", \"TDOA14\",\n",
    "    \"TDOA23\", \"TDOA24\", \"TDOA34\", \"Xcorr12\", \"Xcorr13\", \"Xcorr14\",\n",
    "    \"Xcorr23\", \"Xcorr24\", \"Xcorr34\"\n",
    "]\n",
    "\"\"\"\n",
    "column_names = [\n",
    "    \"PeakTime\", \"Amplitude\", \"DOA_x\", \"DOA_y\", \"DOA_z\", \"TDOA12\", \"TDOA13\", \"TDOA14\",\n",
    "    \"TDOA23\", \"TDOA24\", \"TDOA34\", \"Xcorr12\", \"Xcorr13\", \"Xcorr14\",\n",
    "    \"Xcorr23\", \"Xcorr24\", \"Xcorr34\"\n",
    "]\n",
    "\n",
    "# Replace with your actual data path\n",
    "path_doa = \"../../C/deployment_files/2024-8-29-18-37-36-480140_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-9-25-13-5-15-419096_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-10-1-15-19-31-643613_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-10-1-15-55-32-804596_detection\" #track154\n",
    "#path_doa = \"../2024-9-12-9-36-14-51349_detection\"\n",
    "path_doa = \"../../C/deployment_files/new.txt\"\n",
    "#df = pd.read_csv(path_doa, delim_whitespace=True, names=column_names, skiprows=1)\n",
    "df = pd.read_csv(path_doa, sep=',', names=column_names, skiprows=1)\n",
    "\n",
    "# Instantiate the tracker\n",
    "#tracker = Tracker(eps=3, min_samples=15, missed_update_threshold=4)\n",
    "#tracker = dbscan_module.Tracker(3, 15, 4)\n",
    "print('Before tracker')\n",
    "tracker = dbscan_module.Tracker(0.04, 15, 4)\n",
    "print('After tracker')\n",
    "# Main processing loop (outside the class)\n",
    "#time_intervals = np.linspace(df[\"usec_since_Unix_Start\"].min(), df[\"usec_since_Unix_Start\"].max(), num=20)\n",
    "\n",
    "time_intervals = np.linspace(df[\"PeakTime\"].min(), df[\"PeakTime\"].max(), num=20)\n",
    "grouped_time_intervals = []\n",
    "\n",
    "k=0\n",
    "last_row = 0\n",
    "for ind in range(len(time_intervals) - 1):\n",
    "    # Select the data within the current interval\n",
    "    observations = df[\n",
    "        (df[\"PeakTime\"] > time_intervals[ind]) &\n",
    "        (df[\"PeakTime\"] <= time_intervals[ind + 1])\n",
    "    ]\n",
    "\n",
    "    # If there are no data points in this interval, skip to the next iteration\n",
    "    if observations.empty:\n",
    "        continue\n",
    "    print(\"hello\")\n",
    "    # Run DBSCAN on even intervals\n",
    "    if ind % 2 == 0:\n",
    "        print(\"here\", ind)\n",
    "         \n",
    "        if ind != 0:\n",
    "            for _, row in observations.iterrows():\n",
    "                #observation = [row[\"El.\"], row[\"Az.\"]]\n",
    "                observation = [row[\"DOA_x\"], row[\"DOA_y\"], row[\"DOA_z\"]]\n",
    "                observation = np.squeeze(observation)\n",
    "                tracker.update_kalman_filters_continuous(observation, int(row['PeakTime']))\n",
    "                last_row = row\n",
    "\n",
    "        grouped_time_intervals.append([time_intervals[ind], time_intervals[ind + 1]])\n",
    "        \n",
    "        X = observations[[\"DOA_x\", \"DOA_y\", \"DOA_z\"]].values.astype(np.float32)\n",
    "        tracker.process_batch(X) # Handle batch updates\n",
    "        k = k+1\n",
    "    else:\n",
    "        # For odd intervals, handle individual sample updates\n",
    "        for _, row in observations.iterrows():\n",
    "            observation = [row[\"DOA_x\"], row[\"DOA_y\"], row[\"DOA_z\"]]\n",
    "            observation = np.squeeze(observation)\n",
    "            tracker.update_kalman_filters_continuous(observation, int(row['PeakTime']))\n",
    "            last_row = row\n",
    "\n",
    "print(\"sleeping....\")\n",
    "time.sleep(20)\n",
    "observation = [last_row[\"DOA_x\"], last_row[\"DOA_y\"], last_row[\"DOA_z\"]]\n",
    "observation = np.squeeze(observation)\n",
    "tracker.update_kalman_filters_continuous(observation, int(last_row['PeakTime']))\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf26f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88d912-6d8d-4e31-92be-465ec21a84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tracker_output.csv'\n",
    "\n",
    "# Read the log entries from the CSV file\n",
    "kalman_log = read_log_csv(filename)\n",
    "plot_log(kalman_log)\n",
    "#plot_log(tracker.kalman_log)\n",
    "#print(len(tracker.kalman_filters))\n",
    "#print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24050ea-4ce5-4c37-bb47-9be86b79d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf26104-47ea-4e86-900f-7b2a2c810238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad10304-b2ee-414e-8dec-9dc932da06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls build/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225669dc-70c6-4bf3-8b77-42385c7d765a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybind11_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
