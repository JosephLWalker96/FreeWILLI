{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b803f229-1f61-4be2-bc07-337912c2ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from scipy.spatial.distance import cdist\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('build')  # Add the 'build' directory to the Python path\n",
    "\n",
    "import dbscan_module  # Import the shared object file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4d89df-d675-4ae4-ae40-e9eaee06ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kalman_update_point(cluster_points, cluster_center):\n",
    "    # Assuming cluster_points is a 2D numpy array with x and y coordinates\n",
    "    #print(cluster_points[-5:])\n",
    "    num_points = cluster_points.shape[0]\n",
    "    # Generate a range of colors that go from light to dark\n",
    "    colors = np.linspace(0.3, 1, num_points)  # Adjust 0.3 to 1 for darker shades\n",
    "    \n",
    "    # Plot the cluster points with varying darkness\n",
    "    scatter = plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors, cmap='Greys')\n",
    "    \n",
    "    # Plot the cluster center\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], color='red', marker='x', label='Cluster Center')\n",
    "    \n",
    "    # Add a colorbar, which will automatically use the 'Greys' colormap\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Point Intensity (later points darker)')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_dbscan_output(df, labels):\n",
    "    #print(\"###############################\")\n",
    "    #print(unique_labels)\n",
    "    global global_counter\n",
    "    global_counter +=1\n",
    "    print(\"DBSCAN counter: \",global_counter)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    axs[0,0].scatter(df[\"usec_since_Unix_Start\"], df[\"El.\"], marker='o', c=labels)\n",
    "    axs[0,1].scatter(df[\"usec_since_Unix_Start\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,0].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "\n",
    "    inds = labels > -1\n",
    "    axs[1,1].scatter(df[\"El.\"][inds], df[\"Az.\"][inds], marker='o', c=labels[inds])\n",
    "    # Extract x and y axis limits\n",
    "    x_limits = axs[1, 1].get_xlim()\n",
    "    y_limits = axs[1, 1].get_ylim()\n",
    "    axs[1,1].scatter(df[\"El.\"], df[\"Az.\"], marker='o', c=labels)\n",
    "    axs[1,1].set_xlim(x_limits)\n",
    "    axs[1,1].set_ylim(y_limits)\n",
    "    plt.show()\n",
    "\n",
    "# Extract variables\n",
    "def plot_log(kalman_log):\n",
    "    times = []\n",
    "    filter_ids = []\n",
    "    predicted_xs = []\n",
    "    predicted_ys = []\n",
    "    \n",
    "    for entry in kalman_log:\n",
    "        times.append(entry['time'])\n",
    "        filter_ids.append(entry['filter_id'])\n",
    "        predicted_xs.append(entry['updated_x'])\n",
    "        predicted_ys.append(entry['updated_y'])\n",
    "    \n",
    "    filter_ids = np.array(filter_ids)\n",
    "    unique_ids = np.unique(filter_ids)\n",
    "    print(\"unique tracks: \", unique_ids)\n",
    "    \n",
    "    # Create a custom colormap for only the used unique filter IDs\n",
    "    num_unique = len(unique_ids)  # Number of unique filter IDs\n",
    "    colormap = plt.get_cmap('tab10', num_unique)  # Generate a colormap for the number of unique IDs\n",
    "    \n",
    "    # Normalize the colormap for the unique filter IDs\n",
    "    norm = mcolors.Normalize(vmin=min(unique_ids), vmax=max(unique_ids))\n",
    "    \n",
    "    # Display scatter plot for predicted_ys\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    scatter1 = ax1.scatter(times, predicted_ys, c=filter_ids, cmap=colormap, norm=norm, alpha=0.1)\n",
    "    ax1.set_ylim(-180, 200)\n",
    "    \n",
    "    # Create a ScalarMappable for the color bar to use only the available filter IDs\n",
    "    sm1 = ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm1.set_array([])  # Required for ScalarMappable\n",
    "    \n",
    "    # Add a colorbar with only the colors used in the plot\n",
    "    cbar1 = fig.colorbar(sm1, ax=ax1, ticks=unique_ids, label=\"Track IDs\")\n",
    "    \n",
    "    # Shade the regions specified by grouped_time_intervals\n",
    "    for interval in grouped_time_intervals:\n",
    "        ax1.axvspan(interval[0], interval[1], color='gray', alpha=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Display scatter plot for predicted_xs\n",
    "    fig, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    scatter2 = ax2.scatter(times, predicted_xs, c=filter_ids, cmap=colormap, norm=norm, alpha=0.1)\n",
    "    ax2.set_ylim(30, 200)\n",
    "    \n",
    "    # Create a ScalarMappable for the second color bar to use only the available filter IDs\n",
    "    sm2 = ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm2.set_array([])\n",
    "    \n",
    "    # Add a colorbar with only the colors used in the plot\n",
    "    cbar2 = fig.colorbar(sm2, ax=ax2, ticks=unique_ids, label=\"Track IDs\")\n",
    "    \n",
    "    # Shade the regions specified by grouped_time_intervals\n",
    "    for interval in grouped_time_intervals:\n",
    "        ax2.axvspan(interval[0], interval[1], color='gray', alpha=0.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def PrintInfo(distance_matrix, associations, unassigned_clusters):\n",
    "    print(\"#########################################\")\n",
    "    print(\"distance matrix\")\n",
    "    print(distance_matrix)\n",
    "    print(f\"associations: {associations}\")\n",
    "    print(f\"unassigned_clusters: {unassigned_clusters}\")\n",
    "    for c in unassigned_clusters:\n",
    "        print(\"Initializing new filter for cluster: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36eaaa-da1b-4861-ae47-87027550fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a program that performs multi-target tracking. \n",
    "Initially, clustering (DBSCAN) is performed on a batch of samples to estimate the number of targets. \n",
    "A kalman filter is then initialized for each cluster (i.e. target).\n",
    "New observations are used to either update an existing kalman filter's state or ignored as noise.\n",
    "Periodically, batches of observations are used to perform clustering to update the number of targets. \n",
    "Each cluster is either associated with an existing kalman filter or used to initialize a new kalman filter.\n",
    "If a kalman filter is not assocaited with a cluster for a specified number of clustering iterations, the filter is destroyed.\n",
    "\"\"\"\n",
    "global_counter = 0\n",
    "class KalmanFilter:\n",
    "    def __init__(self,\n",
    "                 transition_matrices,\n",
    "                 observation_matrices,\n",
    "                 initial_state_mean,\n",
    "                 initial_state_covariance,\n",
    "                 transition_covariance,\n",
    "                 observation_covariance):\n",
    "        self.F = np.array(transition_matrices)  # State transition matrix\n",
    "        self.H = np.array(observation_matrices)  # Observation matrix\n",
    "        self.Q = np.array(transition_covariance)  # Process noise covariance\n",
    "        self.R = np.array(observation_covariance)  # Measurement noise covariance\n",
    "        self.x = np.array(initial_state_mean)  # Initial state estimate\n",
    "        self.P = np.array(initial_state_covariance)  # Initial covariance estimate\n",
    "\n",
    "    def predict(self):\n",
    "        # Predict the state estimate and covariance\n",
    "        self.x_prior = self.F @ self.x\n",
    "        self.P_prior = self.F @ self.P @ self.F.T + self.Q\n",
    "\n",
    "    def update(self, z):\n",
    "        # Compute innovation\n",
    "        y = z - self.H @ self.x_prior\n",
    "        # Compute innovation covariance\n",
    "        S = self.H @ self.P_prior @ self.H.T + self.R\n",
    "        # Compute Kalman gain\n",
    "        K = self.P_prior @ self.H.T @ np.linalg.inv(S)\n",
    "        # Update state estimate\n",
    "        self.x = self.x_prior + K @ y\n",
    "        # Update covariance estimate\n",
    "        self.P = (np.eye(self.P_prior.shape[0]) - K @ self.H) @ self.P_prior\n",
    "\n",
    "    def filter_update(self, filtered_state_mean=None, filtered_state_covariance=None, observation=None):\n",
    "        # If filtered_state_mean and covariance are provided, update self.x and self.P\n",
    "        if filtered_state_mean is not None:\n",
    "            self.x = np.array(filtered_state_mean)\n",
    "        if filtered_state_covariance is not None:\n",
    "            self.P = np.array(filtered_state_covariance)\n",
    "        # Predict\n",
    "        self.predict()\n",
    "        # Update if observation is provided\n",
    "        if observation is not None:\n",
    "            self.update(observation)\n",
    "        else:\n",
    "            # If no observation, update x and P with predicted values\n",
    "            self.x = self.x_prior\n",
    "            self.P = self.P_prior\n",
    "        return self.x, self.P\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, eps=3, min_samples=15, missed_update_threshold=4):\n",
    "        # DBSCAN parameters\n",
    "        self.eps = eps  # Max distance between two points to be considered as in the same neighborhood\n",
    "        self.min_samples = min_samples  # Minimum number of points to form a cluster\n",
    "\n",
    "        # Initialize storage for Kalman filters and logs\n",
    "        self.kalman_filters = []\n",
    "        self.cluster_assignments = []  # To track which Kalman filter corresponds to which cluster\n",
    "        self.kalman_log = []  # To log the outputs of the Kalman filter\n",
    "\n",
    "        self.global_counter = 0\n",
    "\n",
    "        self.missed_update_threshold = missed_update_threshold  # Number of iterations a filter can go without being updated before removal\n",
    "        self.missed_updates = []  # Track how many iterations each filter has missed being associated with a cluster\n",
    "        self.next_label = 0\n",
    "\n",
    "    # Function to initialize a Kalman filter given the initial position\n",
    "    def initialize_kalman_filter(self, initial_state):\n",
    "        # State transition matrix: assuming constant velocity model\n",
    "        transition_matrix = np.array([\n",
    "            [1, 0, 1, 0],  # X -> X + Vx\n",
    "            [0, 1, 0, 1],  # Y -> Y + Vy\n",
    "            [0, 0, 1, 0],  # Vx -> Vx\n",
    "            [0, 0, 0, 1]   # Vy -> Vy\n",
    "        ])\n",
    "\n",
    "        # Observation matrix: we can only observe position, not velocity\n",
    "        observation_matrix = np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0]\n",
    "        ])\n",
    "\n",
    "        # Initial guess of the state covariance (uncertainty in initial position and velocity)\n",
    "        initial_state_covariance = np.eye(4) * 1000\n",
    "\n",
    "        # Covariance of the process noise (uncertainty in the motion model)\n",
    "        process_covariance = np.eye(4) * 0.01\n",
    "\n",
    "        # Covariance of the observation noise (uncertainty in the measurements)\n",
    "        observation_covariance = np.eye(2) * 10\n",
    "\n",
    "        # Initial state vector [X, Y, Vx, Vy]\n",
    "        initial_state_mean = np.array([initial_state[0], initial_state[1], 0, 0])  # Assuming zero initial velocity\n",
    "\n",
    "        # Create the Kalman filter\n",
    "        kf = KalmanFilter(\n",
    "            transition_matrices=transition_matrix,\n",
    "            observation_matrices=observation_matrix,\n",
    "            initial_state_mean=initial_state_mean,\n",
    "            initial_state_covariance=initial_state_covariance,\n",
    "            transition_covariance=process_covariance,\n",
    "            observation_covariance=observation_covariance\n",
    "        )\n",
    "\n",
    "        return kf\n",
    "\n",
    "    # Function to run DBSCAN\n",
    "    def run_dbscan(self, df):\n",
    "        self.global_counter += 1\n",
    "        X = df[[\"El.\", \"Az.\"]].values.astype(np.float32)\n",
    "        #db = DBSCAN(eps=self.eps, min_samples=self.min_samples).fit(X)\n",
    "        #labels = db.labels_\n",
    "        db = dbscan_module.dbscan(X, self.eps, self.min_samples)\n",
    "        labels = dbscan_module.label(db, len(X))\n",
    "        labels = np.array(labels) - 1\n",
    "        #print(labels)\n",
    "        #adsfadsf\n",
    "        ##################################\n",
    "        plot_dbscan_output(df, labels)\n",
    "        ##################################\n",
    "\n",
    "        # Extract cluster end regions\n",
    "        cluster_centers = self.get_cluster_centroids(X, labels)\n",
    "\n",
    "        return cluster_centers\n",
    "\n",
    "    # Function to find optimal association\n",
    "    def find_optimal_association(self, distance_matrix, threshold):\n",
    "        num_kalman_filters, num_new_clusters = distance_matrix.shape\n",
    "\n",
    "        # Step 1: Ignore new clusters where all distances to prior clusters are greater than the threshold\n",
    "        unassociated_new_clusters = []\n",
    "        associated_new_clusters = []\n",
    "        for new_cluster in range(num_new_clusters):\n",
    "            if np.all(distance_matrix[:, new_cluster] > threshold):\n",
    "                unassociated_new_clusters.append(new_cluster)\n",
    "            else:\n",
    "                associated_new_clusters.append(new_cluster)\n",
    "\n",
    "        # Step 2: Assign new clusters to prior clusters based on smallest valid distance\n",
    "        associations = [-1] * num_kalman_filters  # Initialize associations with -1 (no association)\n",
    "        assigned_new_clusters = set()  # To track already assigned new clusters\n",
    "\n",
    "        for klm_ind in range(num_kalman_filters):\n",
    "            min_distance = float('inf')\n",
    "            best_new_cluster = -1\n",
    "            for new_cluster in associated_new_clusters:\n",
    "                if distance_matrix[klm_ind, new_cluster] < threshold and new_cluster not in assigned_new_clusters:\n",
    "                    if distance_matrix[klm_ind, new_cluster] < min_distance:\n",
    "                        min_distance = distance_matrix[klm_ind, new_cluster]\n",
    "                        best_new_cluster = new_cluster\n",
    "            if best_new_cluster != -1:\n",
    "                associations[klm_ind] = best_new_cluster\n",
    "                assigned_new_clusters.add(best_new_cluster)\n",
    "\n",
    "        return associations, unassociated_new_clusters\n",
    "\n",
    "    def calculate_distance_matrix(self, cluster_centers):\n",
    "        distance_matrix = np.zeros((len(self.kalman_filters), len(cluster_centers)))\n",
    "        if len(self.kalman_filters) != 0 and len(cluster_centers) !=0:\n",
    "            #predicted_state_means = np.array([kf.predict().x_prior[:2] for kf in self.kalman_filters])\n",
    "            predicted_state_means = np.array([kf.x_prior[:2] for kf in self.kalman_filters])\n",
    "            # Calculate the distance matrix using broadcasting\n",
    "            distance_matrix = np.linalg.norm(predicted_state_means[:, np.newaxis, :] - cluster_centers[np.newaxis, :, :], axis=2)\n",
    "        return distance_matrix\n",
    "\n",
    "    def destroy_expired_filters(self):\n",
    "        # Create a list of indices for filters to keep (those that haven't missed too many updates)\n",
    "        indices_to_keep = [i for i, count in enumerate(self.missed_updates) if count <= self.missed_update_threshold]\n",
    "\n",
    "        # Filter kalman_filters, cluster_assignments, and missed_updates using the same indices\n",
    "        self.kalman_filters = [self.kalman_filters[i] for i in indices_to_keep]\n",
    "        self.missed_updates = [self.missed_updates[i] for i in indices_to_keep]\n",
    "        self.cluster_assignments = [self.cluster_assignments[i] for i in indices_to_keep]\n",
    "\n",
    "    def get_cluster_centroids(self, X, labels):        \n",
    "        cluster_centers = []\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            if label != -1:  # Exclude noise points\n",
    "                cluster_points = X[labels == label]\n",
    "                cluster_center = cluster_points[-3:].mean(axis=0)  # Use last 3 points for averaging\n",
    "                cluster_centers.append(cluster_center)\n",
    "        return np.array(cluster_centers)\n",
    "\n",
    "    def increment_missed_counter(self, associated_filters):\n",
    "        for i in range(len(self.kalman_filters)):\n",
    "            if i not in associated_filters:\n",
    "                self.missed_updates[i] += 1\n",
    "\n",
    "    def initialize_filters_for_clusters(self, unassigned_clusters, cluster_centers):\n",
    "        for c in unassigned_clusters:\n",
    "            # Initialize a new Kalman filter for this unassigned cluster center\n",
    "            new_kf = self.initialize_kalman_filter(cluster_centers[c])\n",
    "            self.kalman_filters.append(new_kf)\n",
    "\n",
    "            # Assign a new unique label to this filter\n",
    "            self.cluster_assignments.append(self.next_label)\n",
    "            self.next_label += 1\n",
    "\n",
    "            # Initialize the missed update count for the new filter\n",
    "            self.missed_updates.append(0)\n",
    "        \n",
    "    # Function to update or initialize Kalman filters based on DBSCAN results\n",
    "    def update_kalman_filters(self, cluster_centers):\n",
    "        \n",
    "        #Compute the pairwise distance matrix between Kalman filter predictions and cluster centers\n",
    "        distance_matrix = self.calculate_distance_matrix(cluster_centers)\n",
    "\n",
    "        #Solve the optimal assignment using the custom find_optimal_association function\n",
    "        associations, unassigned_clusters = self.find_optimal_association(distance_matrix, self.eps)\n",
    "\n",
    "        # To track filters that are associated with a cluster\n",
    "        associated_filters = set()  \n",
    "        for r, c in enumerate(associations):\n",
    "            if c >= 0:  # Check if cluster was assigned\n",
    "                # Update the Kalman filter with its associated cluster.. may not be necessary\n",
    "                #kf = self.kalman_filters[r]\n",
    "                #kf.update(cluster_centers[c])\n",
    "                associated_filters.add(r)\n",
    "                self.missed_updates[r] = 0  # Reset the missed count\n",
    "\n",
    "        self.initialize_filters_for_clusters(unassigned_clusters, cluster_centers)\n",
    "\n",
    "        PrintInfo(distance_matrix, associations, unassigned_clusters)\n",
    "\n",
    "        # Increment missed_count for all non-associated filters\n",
    "        self.increment_missed_counter(associated_filters)\n",
    "\n",
    "        self.destroy_expired_filters()\n",
    "\n",
    "    # Function to update the associated Kalman filters from a sample\n",
    "    def update_kalman_filters_continuous(self, observation, time):\n",
    "        best_match = None\n",
    "        best_dist = 10.0  # Adjusted the gating_threshold to be consistent\n",
    "\n",
    "        # Find the Kalman filter with the closest prediction to the current observation\n",
    "        for idx, kf in enumerate(self.kalman_filters):\n",
    "            # Predict the next state with the Kalman filter\n",
    "            kf.predict()\n",
    "            predicted_state_mean = kf.H @ kf.x_prior\n",
    "            #predicted_state_mean = kf.x_prior[:2]\n",
    "            \n",
    "            # Calculate the distance from the predicted position to the observation\n",
    "            dist = np.linalg.norm(predicted_state_mean - observation)\n",
    "\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_match = idx\n",
    "\n",
    "        # If a valid match is found, update the corresponding Kalman filter\n",
    "        if best_match is not None:\n",
    "            kf = self.kalman_filters[best_match]\n",
    "            kf.update(observation)\n",
    "\n",
    "            # Log the updated state\n",
    "            self.kalman_log.append({\n",
    "                'time': time,\n",
    "                'filter_id': self.cluster_assignments[best_match],\n",
    "                'updated_x': kf.x[0],\n",
    "                'updated_y': kf.x[1]\n",
    "            })\n",
    "\n",
    "    # Function to handle batch updates\n",
    "    def process_batch(self, df_current):\n",
    "        cluster_centers = self.run_dbscan(df_current)\n",
    "        if len(cluster_centers) == 0:\n",
    "            return  # No valid clusters\n",
    "        self.update_kalman_filters(cluster_centers)\n",
    "\n",
    "\n",
    "# Load the data as per your existing code\n",
    "column_names = [\n",
    "    \"usec_since_Unix_Start\", \"Energy\", \"El.\", \"Az.\", \"TDOA12\", \"TDOA13\", \"TDOA14\",\n",
    "    \"TDOA23\", \"TDOA24\", \"TDOA34\", \"Xcorr12\", \"Xcorr13\", \"Xcorr14\",\n",
    "    \"Xcorr23\", \"Xcorr24\", \"Xcorr34\"\n",
    "]\n",
    "\n",
    "# Replace with your actual data path\n",
    "path_doa = \"../../C/deployment_files/2024-8-29-18-37-36-480140_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-9-25-13-5-15-419096_detection\"\n",
    "path_doa = \"../../C/deployment_files/2024-10-1-15-19-31-643613_detection\"\n",
    "#path_doa = \"../2024-9-12-9-36-14-51349_detection\"\n",
    "df = pd.read_csv(path_doa, delim_whitespace=True, names=column_names, skiprows=1)\n",
    "\n",
    "# Instantiate the tracker\n",
    "tracker = Tracker(eps=3, min_samples=15, missed_update_threshold=4)\n",
    "\n",
    "# Main processing loop (outside the class)\n",
    "time_intervals = np.linspace(df[\"usec_since_Unix_Start\"].min(), df[\"usec_since_Unix_Start\"].max(), num=20)\n",
    "grouped_time_intervals = []\n",
    "\n",
    "for ind in range(len(time_intervals) - 1):\n",
    "    # Select the data within the current interval\n",
    "    observations = df[\n",
    "        (df[\"usec_since_Unix_Start\"] > time_intervals[ind]) &\n",
    "        (df[\"usec_since_Unix_Start\"] <= time_intervals[ind + 1])\n",
    "    ]\n",
    "\n",
    "    # If there are no data points in this interval, skip to the next iteration\n",
    "    if observations.empty:\n",
    "        continue\n",
    "\n",
    "    # Run DBSCAN on even intervals\n",
    "    if ind % 2 == 0:\n",
    "        if ind != 0:\n",
    "            for _, row in observations.iterrows():\n",
    "                observation = [row[\"El.\"], row[\"Az.\"]]\n",
    "                tracker.update_kalman_filters_continuous(observation, row['usec_since_Unix_Start'])\n",
    "\n",
    "        grouped_time_intervals.append([time_intervals[ind], time_intervals[ind + 1]])\n",
    "        tracker.process_batch(observations) # Handle batch updates\n",
    "    else:\n",
    "        # For odd intervals, handle individual sample updates\n",
    "        for _, row in observations.iterrows():\n",
    "            observation = [row[\"El.\"], row[\"Az.\"]]\n",
    "            tracker.update_kalman_filters_continuous(observation, row['usec_since_Unix_Start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88d912-6d8d-4e31-92be-465ec21a84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log(tracker.kalman_log)\n",
    "print(len(tracker.kalman_filters))\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24050ea-4ce5-4c37-bb47-9be86b79d150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf26104-47ea-4e86-900f-7b2a2c810238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad10304-b2ee-414e-8dec-9dc932da06b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225669dc-70c6-4bf3-8b77-42385c7d765a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
